{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9863972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toumi\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5591ced5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\toumi'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b50b70e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_names = []\n",
    "\n",
    "for dirname, _, filenames in os.walk('C:\\\\Users\\\\toumi\\\\OneDrive\\\\Bureau\\\\ML files\\\\flowers classification\\\\flowers'):\n",
    "    for filename in filenames:\n",
    "        dir_name = os.path.join(dirname, filename)\n",
    "        dir_names.append(dir_name)\n",
    "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], dir_names))\n",
    "dir_names = pd.Series(dir_names, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "\n",
    "# Concatenate filepaths and labels\n",
    "image_df = pd.concat([dir_names, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f5ec2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split( image_df,test_size=0.2, random_state=0,shuffle=True)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4ad6b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3453 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import *\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "train_images = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(150, 150),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2fc401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 864 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import *\n",
    "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "\n",
    "test_images = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test,\n",
    "    x_col='Filepath',\n",
    "    y_col='Label',\n",
    "    target_size=(150, 150),\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=100,\n",
    "    subset='training'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0250ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers  import Dense,InputLayer,Conv2D,MaxPool2D,Flatten, BatchNormalization, Dropout\n",
    "model = tf.keras.Sequential([\n",
    "\n",
    "\n",
    "#filters=6, kernal_size=5, strides=1, padding='valid', activation='sigmoid'\n",
    "\n",
    "\n",
    "    tf.keras.layers.Conv2D( filters=6,\n",
    "    kernel_size=5,\n",
    "    strides=(1, 1),\n",
    "    padding='valid',\n",
    "    activation='sigmoid', input_shape = (150,150,3)),\n",
    "    MaxPool2D(pool_size=2,strides=2),\n",
    "\n",
    "\n",
    "    tf.keras.layers.Conv2D( filters=16,\n",
    "    kernel_size=5,\n",
    "    strides=(1, 1),\n",
    "    padding='valid',\n",
    "    activation='sigmoid',),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=2,strides=2),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(100, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10 , activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(5 , activation='sigmoid'),\n",
    "\n",
    "])\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6964a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load last weights and note back from 0 \n",
    "model.load_weights(\"C:\\\\Users\\\\toumi\\\\OneDrive\\\\Bureau\\\\rayen staff\\\\weights_best.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7f9b46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "model.compile(optimizer= Adam(learning_rate=0.001),\n",
    "              loss=CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e064bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint callback\n",
    "filepath=\"C:\\\\Users\\\\toumi\\\\OneDrive\\\\Bureau\\\\rayen staff\\\\weights_best.hdf5\"\n",
    "checkpoint=tf.keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                     monitor='val_accuracy', \n",
    "                                     verbose=0, \n",
    "                                     save_best_only=False,\n",
    "                                     save_weights_only=False, \n",
    "                                     mode='max', \n",
    "                                     save_freq='epoch')\n",
    "callbacks_list=[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1a59cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#early stopping\n",
    "early_stoping= tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', min_delta=0, patience=2, verbose=1,\n",
    "    mode='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26d8077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv logger\n",
    "CSvlogger=tf.keras.callbacks.CSVLogger('training.log', \n",
    "                             separator=',', \n",
    "                             append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5836dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "\n",
    "  if epoch <= 1:\n",
    "    learning_rate = lr\n",
    "  else:\n",
    "    learning_rate = lr * tf.math.exp(-0.1)\n",
    "    learning_rate = learning_rate.numpy()\n",
    "\n",
    "\n",
    "  return learning_rate\n",
    "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c46210",
   "metadata": {},
   "outputs": [],
   "source": [
    "  with train_writer.as_default():\n",
    "    tf.summary.scalar('Learning Rate', data = learning_rate, step = epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d473e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/2\n",
      "108/108 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9571"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\toumi\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 173s 2s/step - loss: 0.1334 - accuracy: 0.9571 - lr: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/2\n",
      "108/108 [==============================] - 123s 1s/step - loss: 0.1324 - accuracy: 0.9557 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(\n",
    "                           train_images,\n",
    "                           epochs=2,                        # nombre de boucle à réaliser sur le jeu de données complet\n",
    "                           verbose=1,\n",
    "                           batch_size = 128 ,\n",
    "                           callbacks=[CSvlogger,callbacks_list,early_stoping,scheduler_callback]\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ce15c06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 4s 133ms/step - loss: 4.4301 - accuracy: 0.3576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.430054664611816, 0.3576388955116272]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66957324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.1773955374956131,\n",
       "  0.16065047681331635,\n",
       "  0.11085831373929977,\n",
       "  0.11233333498239517,\n",
       "  0.13462036848068237],\n",
       " 'accuracy': [0.9412105679512024,\n",
       "  0.9455546140670776,\n",
       "  0.962641179561615,\n",
       "  0.9635099768638611,\n",
       "  0.9548218846321106]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42d5d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbb12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,30))\n",
    "for i in range(len(test_images)):\n",
    "    tt=np.array(test_images[i][0])\n",
    "\n",
    "    plt.subplot(6,6,i+1)\n",
    "    tt = tt / 255.0\n",
    "    #t = np.clip(tt, 0, 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "   #plt.ylabel('Label for Y-axis')\n",
    "    plt.grid()\n",
    "    plt.imshow(tt[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe35012",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epochs')\n",
    "\n",
    "plt.legend(['train_acurracy','train_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca377101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load csvlogger file and head it\n",
    "import pandas as pd\n",
    "dd=pd.read_csv('training.log')\n",
    "dd.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
